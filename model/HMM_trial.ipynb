{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Description \n",
    "First working attempt at creating an HMM model in Pytorch for smFRET implementations. This will try GPU use. Please note, most of this code is drawn from the git file located at https://github.com/lorenlugosch/pytorch_HMM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting up the venv and imports.\n",
    "Make sure you have all of the necessary packages imported. If not, create a conda venv that has torch downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh/opt/anaconda3/envs/HMM_venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the HMM Model (init)\n",
    "First, we need to define the HMM model. We will need to initialize the three different dataframes that we need too: priors, transitions, and emissions. \n",
    "### Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(torch.nn.Module): # Torch documentation suggests inheritance from torch.nn.Module\n",
    "  \"\"\"\n",
    "  Hidden Markov Model with discrete observations.\n",
    "  \"\"\"\n",
    "  def __init__(self, transitions, emissions, priors):\n",
    "    \"\"\"\n",
    "    Initializes a new HMM model.\n",
    "\n",
    "    NOTE: The variables 'transitions', 'emissions', and 'priors' should be of type lists. \n",
    "    They will be normalized using torch.nn.functional softmax functions.\n",
    "\n",
    "    Attributes:\n",
    "      N (int): the number of states\n",
    "      M (int): the number of observations\n",
    "      transition_model (TransitionModel): the transition matrix for this HMM\n",
    "      emission_model (EissionModel): the emission matrix for this HMM\n",
    "      state_priors (torch.nn.Parameter): the prior distribution for this HMM\n",
    "      is_cuda (bool): if a GPU is activated using cuda() for use\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    # First, save the number of observations and the number of states\n",
    "    self.N = len(priors) # number of states\n",
    "    self.M = len(emissions[0]) # number of observations\n",
    "\n",
    "    # For the purposes of sampling and other algos, we will keep inputted probabilities unnormalized and pre-process data as needed.\n",
    "\n",
    "    # Create A\n",
    "    self.unnormalized_trans = TransitionMatrix(self.N, transitions)\n",
    "\n",
    "    # b(x_t)\n",
    "    self.unnormalized_emiss = EmissionMatrix(self.N, self.M, emissions)\n",
    "\n",
    "    # pi\n",
    "    self.unnormalized_sp = torch.nn.Parameter(torch.Tensor(priors))\n",
    " \n",
    "    # use the GPU, for speed\n",
    "    if torch.cuda.is_available(): \n",
    "      self.cuda()\n",
    "      self.is_cuda = True\n",
    "\n",
    "    else: self.is_cuda = False\n",
    "\n",
    "class TransitionMatrix(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  The transition matrix for our HMM model.\n",
    "  \"\"\"\n",
    "  def __init__(self, N, transitions):\n",
    "    \"\"\"\n",
    "    Instantiates a new transition matrix for our HMM model.\n",
    "    \"\"\"\n",
    "    ### Checks to make sure that the number of priors and transitions line up\n",
    "    if len(transitions) != N:\n",
    "      raise ValueError(f'Mismatch in the number of priors and rows/cols in \"transitions\". {N} != {len(transitions)}')\n",
    "\n",
    "    super().__init__()\n",
    "    self.N = N\n",
    "    self.matrix = torch.nn.Parameter(torch.Tensor(transitions))\n",
    "\n",
    "class EmissionMatrix(torch.nn.Module):\n",
    "  def __init__(self, N, M, emissions):\n",
    "    \"\"\"\n",
    "    Instantiates a new emission matrix for our HMM model.\n",
    "    \"\"\"\n",
    "    ### Checks if the number of states and rows of the emissions matrix line up\n",
    "    if len(emissions) != N:\n",
    "      raise ValueError(f'Mismatch in the number of priors and rows in \"emissions\". {N} != {len(emissions)}')\n",
    "\n",
    "    super().__init__()\n",
    "    self.N = N\n",
    "    self.M = M\n",
    "    self.matrix = torch.nn.Parameter(torch.Tensor(emissions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Initialization\n",
    "\n",
    "We will follow along with a model. This model is a simple FRET HMM with 2 states and 2 possible observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## According to the FRET API tutorial, there 2 states 0 and 1, both with the same chance of being x_0.\n",
    "priors = [0.5,0.5]\n",
    "\n",
    "## With 2 states there are 4 transitions. \n",
    "## Usually there are numbers close to 1 along diagonal (the prob of not transitioning is higher) and close to 0 else.\n",
    "transitions = [[0.999999, 1e-6],\n",
    "                [1e-6, 0.999999]]\n",
    "\n",
    "## In this example, we have two states (A and B) and two different observations for emission (0 and 1). States are i's and Emissions are j's\n",
    "# This model shows that if you're in state A, you have a higher chance of emitting 1 and if you're in state B, there will be random emissions.\n",
    "observations = [[0.3, 0.7],\n",
    "                [0.5, 0.5]]\n",
    "\n",
    "# Thus, we have the model:\n",
    "model = HMM(transitions, observations, priors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Defining the sample() function\n",
    "Next, we will write a sample(...) function that will allow us to simulate or sample the model for T time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, T=10):\n",
    "  \"\"\"\n",
    "  This function samples the HMM model, returning the hidden states and what was observable.\n",
    "  \n",
    "  This function also locally normalizes the unnormalized_sp, unnormalized_trans, unnormalized_emiss \n",
    "  using the torch.nn.functional.softmax(...)\n",
    "  \"\"\"\n",
    "  state_priors = torch.nn.functional.softmax(self.unnormalized_sp, dim=0)\n",
    "  emission_matrix = torch.nn.functional.softmax(self.unnormalized_emiss.matrix, dim=1)\n",
    "  transition_matrix = torch.nn.functional.softmax(self.unnormalized_trans.matrix, dim=0)\n",
    "\n",
    "  # sample initial state\n",
    "  z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
    "  z = []\n",
    "  x = []\n",
    "  z.append(z_t)\n",
    "  \n",
    "  for t in range(0,T):\n",
    "    # sample emission\n",
    "    x_t = torch.distributions.categorical.Categorical(emission_matrix[z_t]).sample().item()\n",
    "    x.append(x_t)\n",
    "\n",
    "    # sample transition\n",
    "    z_t = torch.distributions.categorical.Categorical(transition_matrix[:,z_t]).sample().item()\n",
    "    if t < T-1: z.append(z_t)\n",
    "\n",
    "  return x, z\n",
    "\n",
    "# Add the sampling method to our HMM class\n",
    "HMM.sample = sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the HMM:\n",
    "The below code will run the HMM and report states and observations. Note, observation 0 does NOT imply that the object is in state 0. 0 and 1 were used for the sake of encoding the information in an easily indexable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0, 1, 1, 1, 1]\n",
      "z: [1, 0, 1, 1, 1]\n",
      "\n",
      "x: [0, 0, 1, 1, 1]\n",
      "z: [1, 1, 1, 0, 0]\n",
      "\n",
      "x: [1, 1, 1, 1, 0]\n",
      "z: [1, 1, 1, 0, 0]\n",
      "\n",
      "x: [0, 1, 1, 1, 1]\n",
      "z: [0, 0, 1, 0, 0]\n",
      "\n",
      "x: [0, 1, 0, 0, 0]\n",
      "z: [1, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "  sampled_x, sampled_z = model.sample(T=5)\n",
    "  print(\"x:\", sampled_x)\n",
    "  print(\"z:\", sampled_z)\n",
    "  print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Forward Algorithm\n",
    "Now, let's implement the forward algorithm. Note, we will be using the log-domain iteration of the algorithm as it is computationally less expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, T):\n",
    "  \"\"\"\n",
    "  x : IntTensor of shape (batch size, T_max)\n",
    "  T : IntTensor of shape (batch size)\n",
    "\n",
    "  Compute log p(x) for each example in the batch.\n",
    "  T = length of each example \n",
    "\n",
    "  This function also locally normalizes the unnormalized_sp, unnormalized_trans, unnormalized_emiss \n",
    "  using the torch.nn.functional.log_softmax(...)\n",
    "\n",
    "  Worth noting, batch size is just the number of observation <<sequences>> passed to the forward algorithm for probability calculation.\n",
    "  \"\"\"\n",
    "  if self.is_cuda:\n",
    "   x = x.cuda()\n",
    "   T = T.cuda()\n",
    "\n",
    "  batch_size = x.shape[0] # how many sequences we'll be calculating for\n",
    "  T_max = x.shape[1] # the number of time observations\n",
    "\n",
    "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_sp, dim=0) # this log normalizes state priors\n",
    "  log_alpha = torch.zeros(batch_size, T_max, self.N) # creates alpha prob matrix in R3. firs dim is batch or sequence number, second is time observation, and last is states\n",
    "  \n",
    "  if self.is_cuda: \n",
    "    log_alpha = log_alpha.cuda()\n",
    "\n",
    "  # SPECIAL NOTE: self.unnormalized_emiss(x[:,0]) will invoke the function __call__(...) from EmissionMatrix that then implicitly calls emission_model_forward(self, x[:,0])\n",
    "    \n",
    "  log_alpha[:, 0, :] = self.unnormalized_emiss(x[:,0]) + log_state_priors\n",
    "  for t in range(1, T_max):\n",
    "    log_alpha[:, t, :] = self.unnormalized_emiss(x[:,t]) + self.unnormalized_trans(log_alpha[:, t-1, :])\n",
    "\n",
    "  # Select the sum for the final timestep (each x may have different length).\n",
    "  log_sums = log_alpha.logsumexp(dim=2)\n",
    "  log_probs = torch.gather(log_sums, 1, T.view(-1,1) - 1)\n",
    "\n",
    "  return log_probs.exp()\n",
    "\n",
    "def emission_model_forward(self, x_t):\n",
    "  log_emission_matrix = torch.nn.functional.log_softmax(self.matrix, dim=1)\n",
    "  out = log_emission_matrix[:, x_t].transpose(0,1)\n",
    "  return out\n",
    "\n",
    "def transition_model_forward(self, log_alpha):\n",
    "  \"\"\"\n",
    "  log_alpha : Tensor of shape (batch size, N)\n",
    "  Multiply previous timestep's alphas by transition matrix (in log domain)\n",
    "  \"\"\"\n",
    "  log_transition_matrix = torch.nn.functional.log_softmax(self.matrix, dim=0)\n",
    "\n",
    "  # Matrix multiplication in the log domain\n",
    "  out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
    "  return out\n",
    "\n",
    "def log_domain_matmul(log_A, log_B):\n",
    "\t\"\"\"\n",
    "\tlog_A : m x n\n",
    "\tlog_B : n x p\n",
    "\toutput : m x p matrix\n",
    "\n",
    "\tNormally, a matrix multiplication\n",
    "\tcomputes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
    "\n",
    "\tA log domain matrix multiplication\n",
    "\tcomputes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
    "\t\"\"\"\n",
    "\tm = log_A.shape[0]\n",
    "\tn = log_A.shape[1]\n",
    "\tp = log_B.shape[1]\n",
    "\n",
    "\tlog_A_expanded = torch.reshape(log_A, (m,n,1))\n",
    "\tlog_B_expanded = torch.reshape(log_B, (1,n,p))\n",
    "\n",
    "\telementwise_sum = log_A_expanded + log_B_expanded\n",
    "\tout = torch.logsumexp(elementwise_sum, dim=1)\n",
    "\n",
    "\treturn out\n",
    "\n",
    "TransitionMatrix.forward = transition_model_forward\n",
    "EmissionMatrix.forward = emission_model_forward\n",
    "HMM.forward = forward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, let's go ahead and calculate the probabilities of all length-3 observations sequences per our model.\n",
    "\n",
    "If our model is working, the sum of these probabilities should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0928],\n",
      "        [0.1114],\n",
      "        [0.1108],\n",
      "        [0.1114],\n",
      "        [0.1356],\n",
      "        [0.1356],\n",
      "        [0.1350],\n",
      "        [0.1673]], grad_fn=<ExpBackward0>)\n",
      "Length-3 Sequences Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "sequences = [[0,0,0], [1,0,0], [0,1,0], [0,0,1], [1,1,0], [0,1,1], [1,0,1], [1,1,1]]\n",
    "x = torch.stack([torch.tensor(x) for x in sequences])\n",
    "T = torch.tensor([3 for x in sequences])\n",
    "p_sequences = model.forward(x, T)\n",
    "\n",
    "print(p_sequences)\n",
    "print(f'Length-3 Sequences Sum: {sum(p_sequences).item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Viterbi Analysis\n",
    "Next, we will implement the viterbi analysis algorithm to calculate the most likely state sequence given the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(self, x, T):\n",
    "  \"\"\"\n",
    "  x : IntTensor of shape (batch size, T_max)\n",
    "  T : IntTensor of shape (batch size)\n",
    "  Find argmax_z log p(x|z) for each (x) in the batch.\n",
    "  \"\"\"\n",
    "  if self.is_cuda:\n",
    "    x = x.cuda()\n",
    "    T = T.cuda()\n",
    "\n",
    "  batch_size = x.shape[0] \n",
    "  T_max = x.shape[1]\n",
    "\n",
    "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_sp, dim=0)\n",
    "  log_delta = torch.zeros(batch_size, T_max, self.N).float()\n",
    "  psi = torch.zeros(batch_size, T_max, self.N).long()\n",
    "  \n",
    "  if self.is_cuda:\n",
    "    log_delta = log_delta.cuda()\n",
    "    psi = psi.cuda()\n",
    "\n",
    "  log_delta[:, 0, :] = self.unnormalized_emiss(x[:,0]) + log_state_priors\n",
    "  for t in range(1, T_max):\n",
    "    max_val, argmax_val = self.unnormalized_trans.maxmul(log_delta[:, t-1, :])\n",
    "    log_delta[:, t, :] = self.unnormalized_emiss(x[:,t]) + max_val\n",
    "    psi[:, t, :] = argmax_val\n",
    "\n",
    "  # Get the log probability of the best path\n",
    "  log_max = log_delta.max(dim=2)[0]\n",
    "  best_path_scores = torch.gather(log_max, 1, T.view(-1,1) - 1)\n",
    "\n",
    "  # This next part is a bit tricky to parallelize across the batch,\n",
    "  # so we will do it separately for each example.\n",
    "  z_star = []\n",
    "  for i in range(0, batch_size):\n",
    "    z_star_i = [ log_delta[i, T[i] - 1, :].max(dim=0)[1].item() ]\n",
    "    for t in range(T[i] - 1, 0, -1):\n",
    "      z_t = psi[i, t, z_star_i[0]].item()\n",
    "      z_star_i.insert(0, z_t)\n",
    "\n",
    "    z_star.append(z_star_i)\n",
    "\n",
    "  return z_star, best_path_scores # return both the best path and its log probability\n",
    "\n",
    "def transition_model_maxmul(self, log_alpha):\n",
    "  log_transition_matrix = torch.nn.functional.log_softmax(self.matrix, dim=0)\n",
    "\n",
    "  out1, out2 = maxmul(log_transition_matrix, log_alpha.transpose(0,1))\n",
    "  return out1.transpose(0,1), out2.transpose(0,1)\n",
    "\n",
    "def maxmul(log_A, log_B):\n",
    "\t\"\"\"\n",
    "\tlog_A : m x n\n",
    "\tlog_B : n x p\n",
    "\toutput : m x p matrix\n",
    "\n",
    "\tSimilar to the log domain matrix multiplication,\n",
    "\tthis computes out_{i,j} = max_k log_A_{i,k} + log_B_{k,j}\n",
    "\t\"\"\"\n",
    "\tm = log_A.shape[0]\n",
    "\tn = log_A.shape[1]\n",
    "\tp = log_B.shape[1]\n",
    "\n",
    "\tlog_A_expanded = torch.stack([log_A] * p, dim=2)\n",
    "\tlog_B_expanded = torch.stack([log_B] * m, dim=0)\n",
    "\n",
    "\telementwise_sum = log_A_expanded + log_B_expanded\n",
    "\tout1,out2 = torch.max(elementwise_sum, dim=1)\n",
    "\n",
    "\treturn out1,out2\n",
    "\n",
    "TransitionMatrix.maxmul = transition_model_maxmul\n",
    "HMM.viterbi = viterbi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, let's go ahead and calculate the most probable state sequencing of seeing the observation sequence [0,0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Probable State Sequence: [0, 0, 0] with probability 0.05734225735068321\n"
     ]
    }
   ],
   "source": [
    "x = torch.stack( [torch.tensor([1,1,1])])\n",
    "T = torch.tensor([3])\n",
    "viterb_analysis = model.viterbi(x, T)\n",
    "print(f'Most Probable State Sequence: {viterb_analysis[0][0]} with probability {torch.exp(viterb_analysis[1][0][0])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Letting the HMM Learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5a: Let's create a Trainer for our model. This trainer will handle training and testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # for displaying progress bar\n",
    "\n",
    "class Trainer:\n",
    "  def __init__(self, model, lr):\n",
    "    # Use Adam optimizer algorithm.\n",
    "    self.model = model\n",
    "    self.lr = lr\n",
    "    self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr, weight_decay=0.00001)\n",
    "  \n",
    "  def train(self, dataset):\n",
    "    train_loss = 0\n",
    "    num_samples = 0\n",
    "    self.model.train()\n",
    "    print_interval = 50\n",
    "    for idx, batch in enumerate(tqdm(dataset.loader)):\n",
    "      x,T = batch\n",
    "      batch_size = len(x)\n",
    "      num_samples += batch_size\n",
    "      log_probs = self.model(x,T)\n",
    "\n",
    "      loss = -log_probs.mean() \n",
    "      # this step is key. we want to maximize the log probablity, or minimize the negative log probability mean of the forward algorithm.\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "      train_loss += loss.cpu().data.numpy().item() * batch_size\n",
    "      if idx % print_interval == 0:\n",
    "        print(\"loss:\", loss.item())\n",
    "        for _ in range(5):\n",
    "          sampled_x, sampled_z = self.model.sample()\n",
    "          print(sampled_x)\n",
    "          print(sampled_z)\n",
    "    train_loss /= num_samples\n",
    "    return train_loss\n",
    "\n",
    "  def test(self, dataset):\n",
    "    test_loss = 0\n",
    "    num_samples = 0\n",
    "    self.model.eval()\n",
    "    print_interval = 50\n",
    "    for idx, batch in enumerate(dataset.loader):\n",
    "      x,T = batch\n",
    "      batch_size = len(x)\n",
    "      num_samples += batch_size\n",
    "      log_probs = self.model(x,T)\n",
    "      loss = -log_probs.mean()\n",
    "      test_loss += loss.cpu().data.numpy().item() * batch_size\n",
    "      if idx % print_interval == 0:\n",
    "        print(\"loss:\", loss.item())\n",
    "        sampled_x, sampled_z = self.model.sample()\n",
    "        print(sampled_x)\n",
    "        print(sampled_z)\n",
    "    test_loss /= num_samples\n",
    "    return test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5b: Let's make the proper dataset classes for the pytorch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class PhotonDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, lines):\n",
    "    self.lines = lines # list of observations of length 3\n",
    "    collate = Collate() # function for generating a minibatch from strings\n",
    "    self.loader = torch.utils.data.DataLoader(self, batch_size=1024, num_workers=0, shuffle=True, collate_fn=collate)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.lines)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    line = self.lines[idx]\n",
    "    return line\n",
    "\n",
    "class Collate:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    \"\"\"\n",
    "    Returns a minibatch of strings, padded to have the same length.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    batch_size = len(batch)\n",
    "    for index in range(batch_size):\n",
    "      x.append(batch[index])\n",
    "\n",
    "    # pad all sequences with 0 to have same length\n",
    "    x_lengths = [len(x_) for x_ in x]\n",
    "    T = max(x_lengths)\n",
    "    for index in range(batch_size):\n",
    "      x[index] += [0] * (T - len(x[index]))\n",
    "      x[index] = torch.tensor(x[index])\n",
    "\n",
    "    # stack into single tensor\n",
    "    x = torch.stack(x)\n",
    "    x_lengths = torch.tensor(x_lengths)\n",
    "    return (x,x_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Epoch 1 of 10 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.11327262967824936\n",
      "[0, 0, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[0, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[0, 1, 1, 1, 0, 1, 0, 0, 1, 1]\n",
      "[1, 1, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 0, 0, 0, 1, 1, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "loss: -0.11980458348989487\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "[1, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "========= Results: epoch 1 of 10 =========\n",
      "train loss: -0.11| valid loss: -0.12\n",
      "\n",
      "========= Epoch 2 of 10 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 86.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.11458279937505722\n",
      "[0, 0, 0, 1, 1, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "[1, 0, 0, 1, 0, 0, 0, 1, 0, 1]\n",
      "[1, 0, 0, 1, 1, 1, 1, 1, 0, 0]\n",
      "[1, 1, 1, 1, 1, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 1]\n",
      "[1, 1, 1, 1, 0, 1, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 1, 1]\n",
      "loss: -0.12049078196287155\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
      "========= Results: epoch 2 of 10 =========\n",
      "train loss: -0.11| valid loss: -0.12\n",
      "\n",
      "========= Epoch 3 of 10 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 90.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.11590242385864258\n",
      "[1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n",
      "[1, 0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
      "[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 1, 0, 0, 1, 0, 1, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[1, 0, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "[1, 1, 1, 1, 0, 0, 1, 1, 1, 1]\n",
      "[1, 0, 0, 1, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "loss: -0.12115065008401871\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "========= Results: epoch 3 of 10 =========\n",
      "train loss: -0.12| valid loss: -0.12\n",
      "\n",
      "========= Epoch 4 of 10 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 89.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.11723162978887558\n",
      "[1, 0, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "[1, 0, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 0, 1, 1, 1, 1, 0]\n",
      "[1, 1, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "[1, 1, 1, 0, 0, 0, 0, 1, 0, 0]\n",
      "[1, 1, 1, 0, 1, 1, 0, 1, 1, 0]\n",
      "[0, 0, 0, 1, 1, 0, 1, 1, 1, 1]\n",
      "loss: -0.12178366631269455\n",
      "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "[1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "========= Results: epoch 4 of 10 =========\n",
      "train loss: -0.12| valid loss: -0.12\n",
      "\n",
      "========= Epoch 5 of 10 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 91.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.1185707226395607\n",
      "[0, 1, 1, 1, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 0]\n",
      "[0, 1, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n",
      "[0, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 1, 1, 0, 0, 1, 1]\n",
      "[0, 1, 1, 1, 1, 1, 0, 1, 0, 0]\n",
      "[1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "loss: -0.12238910049200058\n",
      "[0, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "========= Results: epoch 5 of 10 =========\n",
      "train loss: -0.12| valid loss: -0.12\n",
      "\n",
      "========= Epoch 6 of 10 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 92.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.11991970986127853\n",
      "[0, 1, 0, 1, 1, 1, 1, 0, 1, 0]\n",
      "[1, 1, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 0, 0, 1, 1, 1, 1, 1, 1, 0]\n",
      "[1, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "[0, 1, 1, 1, 1, 1, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 1, 1, 1, 0]\n",
      "[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
      "[0, 0, 0, 1, 1, 0, 1, 1, 1, 0]\n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 0, 0]\n",
      "loss: -0.12296634912490845\n",
      "[1, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
      "[0, 0, 1, 0, 0, 0, 1, 1, 0, 0]\n",
      "========= Results: epoch 6 of 10 =========\n",
      "train loss: -0.12| valid loss: -0.12\n",
      "\n",
      "========= Epoch 7 of 10 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 93.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.12127882242202759\n",
      "[1, 0, 1, 0, 1, 1, 0, 0, 0, 1]\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "[1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n",
      "[1, 1, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "[1, 0, 0, 0, 1, 1, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
      "loss: -0.12351491302251816\n",
      "[1, 0, 0, 1, 0, 0, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "========= Results: epoch 7 of 10 =========\n",
      "train loss: -0.12| valid loss: -0.12\n",
      "\n",
      "========= Epoch 8 of 10 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 95.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.12264826893806458\n",
      "[0, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n",
      "[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 1, 1, 0, 0, 0, 1, 1]\n",
      "[0, 0, 0, 1, 1, 0, 1, 1, 1, 0]\n",
      "[1, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 1, 1, 1, 0, 1, 0, 0, 0, 0]\n",
      "[1, 1, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "loss: -0.12403419613838196\n",
      "[1, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "========= Results: epoch 8 of 10 =========\n",
      "train loss: -0.12| valid loss: -0.12\n",
      "\n",
      "========= Epoch 9 of 10 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 92.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.12402823567390442\n",
      "[1, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
      "[1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "[1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "[1, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n",
      "[1, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[0, 1, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "[1, 1, 1, 0, 0, 0, 0, 1, 0, 1]\n",
      "loss: -0.1245235800743103\n",
      "[1, 0, 0, 0, 1, 0, 1, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "========= Results: epoch 9 of 10 =========\n",
      "train loss: -0.12| valid loss: -0.12\n",
      "\n",
      "========= Epoch 10 of 10 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 93.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -0.12541882693767548\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 1, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 1, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[1, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 1, 1, 1, 0, 1, 1, 0, 0]\n",
      "[0, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n",
      "loss: -0.12498259544372559\n",
      "[0, 1, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "========= Results: epoch 10 of 10 =========\n",
      "train loss: -0.13| valid loss: -0.12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "\n",
    "## According to the FRET API tutorial, there 2 states 0 and 1, both with the same chance of being x_0.\n",
    "priors = [0.5,0.5]\n",
    "\n",
    "## With 2 states there are 4 transitions. \n",
    "## Usually there are numbers close to 1 along diagonal (the prob of not transitioning is higher) and close to 0 else.\n",
    "transitions = [[0.999999, 1e-6],\n",
    "                [1e-6, 0.999999]]\n",
    "\n",
    "## In this example, we have two states (A and B) and two different observations for emission (0 and 1). States are i's and Emissions are j's\n",
    "# This model shows that if you're in state A, you have a higher chance of emitting 1 and if you're in state B, there will be random emissions.\n",
    "observations = [[0.3, 0.7],\n",
    "                [0.5, 0.5]]\n",
    "\n",
    "# Thus, we have the model:\n",
    "model = HMM(transitions, observations, priors)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "trainer = Trainer(model, lr=0.01)\n",
    "train_dataset = PhotonDataset([[0,0,0],[0,0,1],[1,1,0]])\n",
    "valid_dataset = PhotonDataset([[1,0,0],[0,1,0],[1,0,1]])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        print(\"========= Epoch %d of %d =========\" % (epoch+1, num_epochs))\n",
    "        train_loss = trainer.train(train_dataset)\n",
    "        valid_loss = trainer.test(valid_dataset)\n",
    "\n",
    "        print(\"========= Results: epoch %d of %d =========\" % (epoch+1, num_epochs))\n",
    "        print(\"train loss: %.2f| valid loss: %.2f\\n\" % (train_loss, valid_loss) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the training process went!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi State Path: [[1, 1, 1]] with probability of observation being modeled by this AI: 0.03617401793599129\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0,1,1]])\n",
    "T = torch.tensor([3])\n",
    "print(f'Viterbi State Path: {model.viterbi(x,T)[0]} with probability of observation being modeled by this AI: {model.viterbi(x,T)[1][0].exp().item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the training dataset is small...but we're ready for bigger fish!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HMM_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d622ee3689cd504a23a24f27e6646d1b829f6dc3a0c1c0a17cc9f259ab39791"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
