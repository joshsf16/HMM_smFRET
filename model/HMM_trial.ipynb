{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Description \n",
    "First working attempt at creating an HMM model in Pytorch for smFRET implementations. This will try GPU use. Please note, most of this code is drawn from the git file located at https://github.com/lorenlugosch/pytorch_HMM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting up the venv and imports.\n",
    "Make sure you have all of the necessary packages imported. If not, create a conda venv that has torch downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the HMM Model (init)\n",
    "First, we need to define the HMM model. We will need to initialize the three different dataframes that we need too: priors, transitions, and emissions. \n",
    "### Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(torch.nn.Module): # Torch documentation suggests inheritance from torch.nn.Module\n",
    "  \"\"\"\n",
    "  Hidden Markov Model with discrete observations.\n",
    "  \"\"\"\n",
    "  def __init__(self, transitions, emissions, priors):\n",
    "    \"\"\"\n",
    "    Initializes a new HMM model.\n",
    "\n",
    "    NOTE: The variables 'transitions', 'emissions', and 'priors' should be of type lists. \n",
    "    They will be normalized using torch.nn.functional softmax functions.\n",
    "\n",
    "    Attributes:\n",
    "      N (int): the number of states\n",
    "      M (int): the number of observations\n",
    "      transition_model (TransitionModel): the transition matrix for this HMM\n",
    "      emission_model (EissionModel): the emission matrix for this HMM\n",
    "      state_priors (torch.nn.Parameter): the prior distribution for this HMM\n",
    "      is_cuda (bool): if a GPU is activated using cuda() for use\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    # First, save the number of observations and the number of states\n",
    "    self.N = len(priors) # number of states\n",
    "    self.M = len(emissions[0]) # number of observations\n",
    "\n",
    "    # For the purposes of sampling and other algos, we will keep inputted probabilities unnormalized and pre-process data as needed.\n",
    "\n",
    "    # Create A\n",
    "    self.unnormalized_trans = TransitionMatrix(self.N, transitions)\n",
    "\n",
    "    # b(x_t)\n",
    "    self.unnormalized_emiss = EmissionMatrix(self.N, self.M, emissions)\n",
    "\n",
    "    # pi\n",
    "    self.unnormalized_sp = torch.nn.Parameter(torch.Tensor(priors))\n",
    " \n",
    "    # use the GPU, for speed\n",
    "    if torch.cuda.is_available(): \n",
    "      self.cuda()\n",
    "      self.is_cuda = True\n",
    "\n",
    "    else: self.is_cuda = False\n",
    "\n",
    "class TransitionMatrix(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  The transition matrix for our HMM model.\n",
    "  \"\"\"\n",
    "  def __init__(self, N, transitions):\n",
    "    \"\"\"\n",
    "    Instantiates a new transition matrix for our HMM model.\n",
    "    \"\"\"\n",
    "    ### Checks to make sure that the number of priors and transitions line up\n",
    "    if len(transitions) != N:\n",
    "      raise ValueError(f'Mismatch in the number of priors and rows/cols in \"transitions\". {N} != {len(transitions)}')\n",
    "\n",
    "    super().__init__()\n",
    "    self.N = N\n",
    "    self.matrix = torch.nn.Parameter(torch.Tensor(transitions))\n",
    "\n",
    "class EmissionMatrix(torch.nn.Module):\n",
    "  def __init__(self, N, M, emissions):\n",
    "    \"\"\"\n",
    "    Instantiates a new emission matrix for our HMM model.\n",
    "    \"\"\"\n",
    "    ### Checks if the number of states and rows of the emissions matrix line up\n",
    "    if len(emissions) != N:\n",
    "      raise ValueError(f'Mismatch in the number of priors and rows in \"emissions\". {N} != {len(emissions)}')\n",
    "\n",
    "    super().__init__()\n",
    "    self.N = N\n",
    "    self.M = M\n",
    "    self.matrix = torch.nn.Parameter(torch.Tensor(emissions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Initialization\n",
    "\n",
    "We will follow along with a model. This model is a simple FRET HMM with 2 states and 2 possible observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## According to the FRET API tutorial, there 2 states 0 and 1, both with the same chance of being x_0.\n",
    "priors = [0.5,0.5]\n",
    "\n",
    "## With 2 states there are 4 transitions. \n",
    "## Usually there are numbers close to 1 along diagonal (the prob of not transitioning is higher) and close to 0 else.\n",
    "transitions = [[0.999999, 1e-6],\n",
    "                [1e-6, 0.999999]]\n",
    "\n",
    "## In this example, we have two states (A and B) and two different observations for emission (0 and 1). States are i's and Emissions are j's\n",
    "# This model shows that if you're in state A, you have a higher chance of emitting 1 and if you're in state B, there will be random emissions.\n",
    "observations = [[0.3, 0.7],\n",
    "                [0.5, 0.5]]\n",
    "\n",
    "# Thus, we have the model:\n",
    "model = HMM(transitions, observations, priors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Defining the sample() function\n",
    "Next, we will write a sample(...) function that will allow us to simulate or sample the model for T time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, T=10):\n",
    "  \"\"\"\n",
    "  This function samples the HMM model, returning the hidden states and what was observable.\n",
    "  \n",
    "  This function also locally normalizes the unnormalized_sp, unnormalized_trans, unnormalized_emiss \n",
    "  using the torch.nn.functional.softmax(...)\n",
    "  \"\"\"\n",
    "  state_priors = torch.nn.functional.softmax(self.unnormalized_sp, dim=0)\n",
    "  emission_matrix = torch.nn.functional.softmax(self.unnormalized_emiss.matrix, dim=1)\n",
    "  transition_matrix = torch.nn.functional.softmax(self.unnormalized_trans.matrix, dim=0)\n",
    "\n",
    "  # sample initial state\n",
    "  z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
    "  z = []\n",
    "  x = []\n",
    "  z.append(z_t)\n",
    "  \n",
    "  for t in range(0,T):\n",
    "    # sample emission\n",
    "    x_t = torch.distributions.categorical.Categorical(emission_matrix[z_t]).sample().item()\n",
    "    x.append(x_t)\n",
    "\n",
    "    # sample transition\n",
    "    z_t = torch.distributions.categorical.Categorical(transition_matrix[:,z_t]).sample().item()\n",
    "    if t < T-1: z.append(z_t)\n",
    "\n",
    "  return x, z\n",
    "\n",
    "# Add the sampling method to our HMM class\n",
    "HMM.sample = sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the HMM:\n",
    "The below code will run the HMM and report states and observations. Note, observation 0 does NOT imply that the object is in state 0. 0 and 1 were used for the sake of encoding the information in an easily indexable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1, 1, 0, 0, 0]\n",
      "z: [0, 0, 0, 0, 1]\n",
      "\n",
      "x: [1, 1, 1, 0, 1]\n",
      "z: [0, 1, 0, 0, 0]\n",
      "\n",
      "x: [0, 0, 1, 1, 1]\n",
      "z: [0, 1, 0, 0, 0]\n",
      "\n",
      "x: [1, 1, 1, 1, 0]\n",
      "z: [1, 1, 0, 0, 0]\n",
      "\n",
      "x: [1, 0, 0, 1, 0]\n",
      "z: [0, 0, 0, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "  sampled_x, sampled_z = model.sample(T=5)\n",
    "  print(\"x:\", sampled_x)\n",
    "  print(\"z:\", sampled_z)\n",
    "  print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Forward Algorithm\n",
    "Now, let's implement the forward algorithm. Note, we will be using the log-domain iteration of the algorithm as it is computationally less expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, T):\n",
    "  \"\"\"\n",
    "  x : IntTensor of shape (batch size, T_max)\n",
    "  T : IntTensor of shape (batch size)\n",
    "\n",
    "  Compute log p(x) for each example in the batch.\n",
    "  T = length of each example \n",
    "\n",
    "  This function also locally normalizes the unnormalized_sp, unnormalized_trans, unnormalized_emiss \n",
    "  using the torch.nn.functional.log_softmax(...)\n",
    "\n",
    "  Worth nothing, batch size is just the number of observation <<sequences>> passed to the forward algorith for probability calculation.\n",
    "  \"\"\"\n",
    "  if self.is_cuda:\n",
    "   x = x.cuda()\n",
    "   T = T.cuda()\n",
    "\n",
    "  batch_size = x.shape[0] # how many sequences we'll be calculating for\n",
    "  T_max = x.shape[1] # the number of time observations\n",
    "\n",
    "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_sp, dim=0) # this log normalizes state priors\n",
    "  log_alpha = torch.zeros(batch_size, T_max, self.N) # creates alpha prob matrix in R3. firs dim is batch or sequence number, second is time observation, and last is states\n",
    "  \n",
    "  if self.is_cuda: \n",
    "    log_alpha = log_alpha.cuda()\n",
    "\n",
    "  # SPECIAL NOTE: self.unnormalized_emiss(x[:,0]) will invoke the function __call__(...) from EmissionMatrix that then implicitly calls emission_model_forward(self, x[:,0])\n",
    "    \n",
    "  log_alpha[:, 0, :] = self.unnormalized_emiss(x[:,0]) + log_state_priors\n",
    "  for t in range(1, T_max):\n",
    "    log_alpha[:, t, :] = self.unnormalized_emiss(x[:,t]) + self.unnormalized_trans(log_alpha[:, t-1, :])\n",
    "\n",
    "  # Select the sum for the final timestep (each x may have different length).\n",
    "  log_sums = log_alpha.logsumexp(dim=2)\n",
    "  log_probs = torch.gather(log_sums, 1, T.view(-1,1) - 1)\n",
    "\n",
    "  return log_probs.exp()\n",
    "\n",
    "def emission_model_forward(self, x_t):\n",
    "  log_emission_matrix = torch.nn.functional.log_softmax(self.matrix, dim=1)\n",
    "  out = log_emission_matrix[:, x_t].transpose(0,1)\n",
    "  return out\n",
    "\n",
    "def transition_model_forward(self, log_alpha):\n",
    "  \"\"\"\n",
    "  log_alpha : Tensor of shape (batch size, N)\n",
    "  Multiply previous timestep's alphas by transition matrix (in log domain)\n",
    "  \"\"\"\n",
    "  log_transition_matrix = torch.nn.functional.log_softmax(self.matrix, dim=0)\n",
    "\n",
    "  # Matrix multiplication in the log domain\n",
    "  out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
    "  return out\n",
    "\n",
    "def log_domain_matmul(log_A, log_B):\n",
    "\t\"\"\"\n",
    "\tlog_A : m x n\n",
    "\tlog_B : n x p\n",
    "\toutput : m x p matrix\n",
    "\n",
    "\tNormally, a matrix multiplication\n",
    "\tcomputes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
    "\n",
    "\tA log domain matrix multiplication\n",
    "\tcomputes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
    "\t\"\"\"\n",
    "\tm = log_A.shape[0]\n",
    "\tn = log_A.shape[1]\n",
    "\tp = log_B.shape[1]\n",
    "\n",
    "\tlog_A_expanded = torch.reshape(log_A, (m,n,1))\n",
    "\tlog_B_expanded = torch.reshape(log_B, (1,n,p))\n",
    "\n",
    "\telementwise_sum = log_A_expanded + log_B_expanded\n",
    "\tout = torch.logsumexp(elementwise_sum, dim=1)\n",
    "\n",
    "\treturn out\n",
    "\n",
    "TransitionMatrix.forward = transition_model_forward\n",
    "EmissionMatrix.forward = emission_model_forward\n",
    "HMM.forward = forward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, let's go ahead and calculate the probabilities of all length-3 observations sequences per our model.\n",
    "\n",
    "If our model is working, the sum of these probabilities should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0928],\n",
      "        [0.1114],\n",
      "        [0.1108],\n",
      "        [0.1114],\n",
      "        [0.1356],\n",
      "        [0.1356],\n",
      "        [0.1350],\n",
      "        [0.1673]], grad_fn=<ExpBackward0>)\n",
      "Length-3 Sequences Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "sequences = [[0,0,0], [1,0,0], [0,1,0], [0,0,1], [1,1,0], [0,1,1], [1,0,1], [1,1,1]]\n",
    "x = torch.stack([torch.tensor(x) for x in sequences])\n",
    "T = torch.tensor([3 for x in sequences])\n",
    "p_sequences = model.forward(x, T)\n",
    "\n",
    "print(p_sequences)\n",
    "print(f'Length-3 Sequences Sum: {sum(p_sequences).item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Viterbi Analysis\n",
    "Next, we will implement the viterbi analysis algorithm to calculate the most likely state sequence given the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HMM_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d622ee3689cd504a23a24f27e6646d1b829f6dc3a0c1c0a17cc9f259ab39791"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
